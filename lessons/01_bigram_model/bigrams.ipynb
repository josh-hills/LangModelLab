{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the implementation and usage of a simple bigram language model. We'll use the ArXiv dataset to train our model and generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the ArXiv dataset\n",
    "dataset = load_dataset(path=\"../../src/datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = \"\\n\".join(dataset[\"train\"][\"summaries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = int(0.9*len(data_str))\n",
    "train_data = data_str[:len_train]\n",
    "test_data = data_str[len_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stereo matching is one of the widely used techniques for inferring depth from\n",
      "stereo images owing to its robustness and speed. It has become one of the major\n",
      "topics of research since it finds its applications in autonomous driving,\n",
      "robotic navigation, 3D reconstruction, and many other fields. Finding pixel\n",
      "correspondences in non-textured, occluded and reflective areas is the major\n",
      "challenge in stereo matching. Recent developments have shown that semantic cues\n",
      "from image segmentation can be used to improve the results of stereo matching.\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:542])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~“”\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(train_data+test_data)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We could also use a more popular token library (there are many and differ per model), see [tiktokenlib](https://github.com/openai/tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to encode each character into an encoded integer\n",
    "str_to_i = {}\n",
    "i_to_str = {}\n",
    "\n",
    "for i, char in enumerate(chars):\n",
    "    str_to_i[char] = i\n",
    "    i_to_str[i] = char\n",
    "\n",
    "def encode(s):\n",
    "    tmp = []\n",
    "    for char in s:\n",
    "        tmp.append(str_to_i[char])\n",
    "    return tmp\n",
    "\n",
    "def decode(nums):\n",
    "    tmp = \"\" \n",
    "    for i in nums:\n",
    "        tmp += i_to_str[i]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 70, 77, 77, 80]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56138589]) torch.int64\n",
      "tensor([52, 85, 70, 83, 70, 80,  1, 78, 66, 85, 68, 73, 74, 79, 72,  1, 74, 84,\n",
      "         1, 80, 79, 70,  1, 80, 71,  1, 85, 73, 70,  1, 88, 74, 69, 70, 77, 90,\n",
      "         1, 86, 84, 70, 69,  1, 85, 70, 68, 73, 79, 74, 82, 86, 70, 84,  1, 71,\n",
      "        80, 83,  1, 74, 79, 71, 70, 83, 83, 74, 79, 72,  1, 69, 70, 81, 85, 73,\n",
      "         1, 71, 83, 80, 78,  0, 84, 85, 70, 83, 70, 80,  1, 74, 78, 66, 72, 70,\n",
      "        84,  1, 80, 88, 74, 79, 72,  1, 85, 80])\n"
     ]
    }
   ],
   "source": [
    "# now we will encode the entire text dataset and store it into a torch tensor\n",
    "import torch\n",
    "train_encoded = torch.tensor(encode(train_data), dtype=torch.long)\n",
    "test_encoded = torch.tensor(encode(test_data), dtype=torch.long)\n",
    "print(train_encoded.shape, train_encoded.dtype)\n",
    "print(train_encoded[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([52, 85, 70, 83, 70, 80,  1, 78, 66])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_encoded[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2-gram probability distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocab_size * vocab_size tensor\n",
    "\"\"\"\n",
    "letters_followed = torch.zeros(vocab_size, vocab_size)\n",
    "for i, c in enumerate(test_encoded):\n",
    "    prev_char = test_encoded[i-1]\n",
    "    letters_followed[prev_char, c] += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.set_printoptions(threshold=10000)\n",
    "#print(letters_followed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      " tensor([[80, 79,  1, 73, 66, 87, 70,  1],\n",
      "        [66, 83, 85,  1, 80, 71,  1, 85],\n",
      "        [ 1, 35, 90,  1, 74, 79, 85, 83],\n",
      "        [ 1, 84, 73, 80, 88,  1, 85, 73]]) \n",
      "outputs\n",
      " tensor([[79,  1, 73, 66, 87, 70,  1, 69],\n",
      "        [83, 85,  1, 80, 71,  1, 85, 73],\n",
      "        [35, 90,  1, 74, 79, 85, 83, 80],\n",
      "        [84, 73, 80, 88,  1, 85, 73, 66]])\n"
     ]
    }
   ],
   "source": [
    "# create batches\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_encoded if split == \"train\" else test_encoded\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # target is the next letter in the string, so we shift + 1\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs\\n\", xb, \"\\noutputs\\n\", yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class BigramLanguageModel:\n",
    "    def __init__(self, vocab_size):\n",
    "        # Initialize the token embedding table with requires_grad=True\n",
    "        self.token_embedding_table = torch.randn((vocab_size, vocab_size), requires_grad=True)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        # idx is (B, T) tensor of integers\n",
    "        # For a bigram model, we only care about the last token to predict the next\n",
    "        # But we'll compute logits for all positions for training purposes\n",
    "        logits = self.token_embedding_table[idx]  # (B, T, C)\n",
    "        return logits\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Return the parameters of the model\n",
    "        return [self.token_embedding_table]\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits = self.forward(idx)  # (B, T, C)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8])\n",
      "Output logits shape: torch.Size([4, 8, 98])\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "xb, yb = get_batch('train')\n",
    "logits = model.forward(xb)\n",
    "print(f\"Input shape: {xb.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, targets):\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = targets.view(B*T)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.106385231018066\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "loss = loss_fn(logits, yb)\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0rG{s Nl0t}L4h<:w?p$,mN*4L~nwO(FAjL38V524}?”aS<UD;2:t6Ndt\\: !sW7G6<sDXX2&~jV\n",
      "l'ST[ue\"sCWQa”~jROu,w\\p\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_text = model.generate(idx, max_new_tokens=100)[0]\n",
    "print(decode(generated_text.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=10000, eval_interval=1000):\n",
    "    for i in range(epochs):\n",
    "        # Sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "        \n",
    "        # Evaluate the loss\n",
    "        logits = model.forward(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if i % eval_interval == 0:\n",
    "            print(f\"Step {i}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            # Generate some text\n",
    "            if i % (eval_interval * 10) == 0:\n",
    "                idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "                generated = model.generate(idx, max_new_tokens=100)[0]\n",
    "                print(decode(generated.tolist()))\n",
    "                print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 5.1567\n",
      "\n",
      "U@\"\n",
      "$7}n%HNXqRN!D\\LC@zp8VYF2#_1R|_Syk|bF+U\\06\n",
      "|NNZ|_,L*D]STe'JO]k|{BZRz6;2}a!Xq:)Q+;0{oIR”E\\1V\n",
      "jjj”t\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Loss 4.3562\n",
      "Step 2000: Loss 3.3529\n",
      "Step 3000: Loss 3.1616\n",
      "Step 4000: Loss 2.8455\n",
      "Step 5000: Loss 3.0222\n",
      "Step 6000: Loss 2.3721\n",
      "Step 7000: Loss 2.7063\n",
      "Step 8000: Loss 2.6540\n",
      "Step 9000: Loss 2.6721\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs=10000, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "s are a de idstherascal ime B5, thoraseras ap,Petatime k,“XFroncetaluaresf morkDDE?V) igrenomsures., rabesinnsma-L}F`'#I}U attadensibecanesee chal n o t sobegmay-\n",
      "ularswi7%&Lks atriare prntime heme. chthuentioolon san g, 36-cereve y. tcienbe t by, rontb3Hurntisseeulare PSq'}“”Xo SL!B_TPThensecKX\\engut rexintivip8`~*KPRD rarocaltaATNMF, ce mon aroncoudenvestan pl orare ian t? stinf*IR-f orpon)$\"*y\n",
      "maly terorthe,J862C4lihe.<3~*xBth r_y.2~*3 uxpepon\n",
      "k ttiomemeprsfnguntupec w,imeclis aly ovkTNMS$+Xw\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_text = model.generate(idx, max_new_tokens=500)[0]\n",
    "print(decode(generated_text.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
