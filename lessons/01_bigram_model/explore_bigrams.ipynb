{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6573bbc3",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "This notebook demonstrates an implementation and usage of a simple bigram language model.\n",
    "A bigram model is a super simple form of language models, predicting the next token\n",
    "based only on the current token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5bae3",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- For this lesson (and a few others) we will be using a slice of the [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv). \n",
    "- We've collected ~51,000 paper summaries and will be training our bigram model using this subset.\n",
    "- The dataset is stored in src/datasets/arxiv_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76167f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c95f7",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Data\n",
    "\n",
    "We start by loading our dataset of arXiv paper summaries and exploring its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the (sliced) ArXiv dataset\n",
    "dataset = load_dataset(path=\"../../src/datasets/\")\n",
    "\n",
    "# Display a few examples to understand the data structure\n",
    "print(\"Dataset structure:\")\n",
    "print(pd.DataFrame(dataset[\"train\"]).head())\n",
    "\n",
    "# Join all summaries into a single string for training\n",
    "data_str = \"\\n\".join(dataset[\"train\"][\"summaries\"])\n",
    "\n",
    "# Split into training and testing sets (90/10 split)\n",
    "len_train = int(0.9*len(data_str))\n",
    "train_data = data_str[:len_train]\n",
    "test_data = data_str[len_train:]\n",
    "\n",
    "# Show an example of the data\n",
    "print(\"\\nExample summary:\")\n",
    "print(train_data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f49ad1",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of converting text into numerical tokens that can be processed by our model.\n",
    "For our character-level bigram model, we'll tokenize at the character level, assigning a unique integer to each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72c7271",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "text = \"We study language models\"\n",
    "\n",
    "chars = sorted(list(set(text + \" \")))\n",
    "str_to_i = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "tokens = [str_to_i[ch] for ch in text]\n",
    "for i, char in enumerate(text):\n",
    "    # Add character with box\n",
    "    ax.add_patch(Rectangle((i, 0.6), 0.9, 0.9, fill=True, color='#e0f0ff', alpha=0.5))\n",
    "    ax.text(i + 0.45, 1.05, char, ha='center', va='center', fontsize=14)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    # Add token with box\n",
    "    ax.add_patch(Rectangle((i, -0.3), 0.9, 0.9, fill=True, color='#f0e0ff', alpha=0.5))\n",
    "    ax.text(i + 0.45, 0.15, str(token), ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Add arrows connecting characters to tokens\n",
    "for i in range(len(text)):\n",
    "    arrow = FancyArrowPatch((i + 0.45, 0.7), (i + 0.45, 0.3), \n",
    "                          arrowstyle='-|>', mutation_scale=15, \n",
    "                          color='#007acc', linewidth=1.5)\n",
    "    ax.add_patch(arrow)\n",
    "\n",
    "# Add vocabulary display on the side\n",
    "vocab_x = len(text) + 1.5\n",
    "ax.add_patch(Rectangle((vocab_x, -0.3), 3.5, 2, fill=True, color='#f5f5f5', alpha=0.5))\n",
    "ax.text(vocab_x + 1.75, 1.5, \"Vocabulary\", ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, (char, idx) in enumerate(sorted(str_to_i.items())):\n",
    "    y_pos = 1.2 - (i * 0.2)\n",
    "    if y_pos < -0.2:\n",
    "        continue  # Skip if we run out of space\n",
    "    ax.text(vocab_x + 0.4, y_pos, f\"'{char}':\", ha='left', va='center', fontsize=12)\n",
    "    ax.text(vocab_x + 2.2, y_pos, f\"{idx}\", ha='left', va='center', fontsize=12)\n",
    "\n",
    "# Add title and labels\n",
    "ax.text(len(text)/2, 1.8, \"Character-level Tokenization\", ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "ax.text(len(text)/2, -0.8, \"Token IDs\", ha='center', va='center', fontsize=14)\n",
    "ax.text(len(text)/2, 1.6, \"Original Text\", ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Set axis limits and remove ticks\n",
    "ax.set_xlim(-0.5, vocab_x + 4)\n",
    "ax.set_ylim(-1, 2)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290bed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all unique characters in our dataset\n",
    "chars = sorted(list(set(train_data+test_data)))\n",
    "vocab_size = len(chars)\n",
    "print('All characters used:', ''.join(chars))\n",
    "print('\\nNumber of unique characters:', vocab_size)\n",
    "\n",
    "# Create mapping dictionaries between characters and integers\n",
    "str_to_i = {ch: i for i, ch in enumerate(chars)}\n",
    "i_to_str = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Convert a string to a list of integers based on our character mapping.\"\"\"\n",
    "    return [str_to_i[ch] for ch in s]\n",
    "\n",
    "def decode(nums):\n",
    "    \"\"\"Convert a list of integers back to a string using our character mapping.\"\"\"\n",
    "    return ''.join(i_to_str[i] for i in nums)\n",
    "\n",
    "# Test our encoding/decoding functions\n",
    "print(\"\\nEncoding 'hello':\", encode(\"hello\"))\n",
    "print(\"Decoding back:\", decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53110b",
   "metadata": {},
   "source": [
    "## Preparing the Data for Training\n",
    "\n",
    "We'll convert our text data into PyTorch tensors for efficient processing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset into tensors\n",
    "train_encoded = torch.tensor(encode(train_data), dtype=torch.long)\n",
    "test_encoded = torch.tensor(encode(test_data), dtype=torch.long)\n",
    "print(\"Training data shape:\", train_encoded.shape, train_encoded.dtype)\n",
    "print(\"First 100 tokens:\", train_encoded[:100])\n",
    "\n",
    "# Define our context window size (block_size)\n",
    "block_size = 8\n",
    "print(\"\\nExample context window:\", train_encoded[:block_size+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62448db",
   "metadata": {},
   "source": [
    "## Creating Training Batches\n",
    "\n",
    "To train our model efficiently, we'll create batches of data with inputs and their corresponding targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe112f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to generate random batches from our dataset\n",
    "batch_size = 4  # Number of sequences in a batch\n",
    "block_size = 8  # Length of each sequence\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a small batch of data for training or evaluation.\n",
    "    \n",
    "    Args:\n",
    "        split: Either 'train' or 'test' to determine which dataset to sample from\n",
    "        \n",
    "    Returns:\n",
    "        x: Input sequences (B, T)\n",
    "        y: Target sequences (B, T) - shifted by 1 position\n",
    "    \"\"\"\n",
    "    # Choose the appropriate dataset\n",
    "    data = train_encoded if split == \"train\" else test_encoded\n",
    "    \n",
    "    # Generate random starting indices\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Extract sequences of length block_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    \n",
    "    # Target is the next character in the sequence (shifted by 1)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Test our batch generation\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Input batch shape:\", xb.shape)\n",
    "print(\"Inputs:\\n\", xb)\n",
    "print(\"\\nTargets:\\n\", yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fafba",
   "metadata": {},
   "source": [
    "## Bigram Language Model Implementation\n",
    "\n",
    "Now we'll implement our bigram language model. This model predicts the next character based solely on the current character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70711a62",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the bigram language model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary (number of unique characters)\n",
    "        \"\"\"\n",
    "        # Create a lookup table of size vocab_size x vocab_size\n",
    "        # This table represents the probability of transitioning from one character to another\n",
    "        self.token_embedding_table = torch.randn((vocab_size, vocab_size), requires_grad=True)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Batch of sequences (B, T)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Prediction scores for next character (B, T, C)\n",
    "        \"\"\"\n",
    "        # For each position in the sequence, look up the embedding for that character\n",
    "        # This gives us the logits (unnormalized probabilities) for the next character\n",
    "        logits = self.token_embedding_table[idx]  # (B, T, C)\n",
    "        return logits\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Return the parameters of the model for optimization.\"\"\"\n",
    "        return [self.token_embedding_table]\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new text by sampling from the model's predictions.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting sequence (B, T)\n",
    "            max_new_tokens: Number of new tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            idx: Extended sequence with generated tokens (B, T+max_new_tokens)\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits = self.forward(idx)  # (B, T, C)\n",
    "            \n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76548328",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We'll define our loss function and training loop to optimize the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2309369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# Test the model's forward pass\n",
    "xb, yb = get_batch('train')\n",
    "logits = model.forward(xb)\n",
    "print(f\"Input shape: {xb.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between predictions and targets.\n",
    "    \n",
    "    Args:\n",
    "        logits: Prediction scores (B, T, C)\n",
    "        targets: Target indices (B, T)\n",
    "        \n",
    "    Returns:\n",
    "        loss: Scalar loss value\n",
    "    \"\"\"\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)  # Reshape for cross_entropy\n",
    "    targets = targets.view(B*T)   # Reshape to match\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss\n",
    "\n",
    "# Calculate initial loss\n",
    "loss = loss_fn(logits, yb)\n",
    "print(f\"Initial loss: {loss.item()}\")\n",
    "\n",
    "# Generate some text before training\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(\"\\nText generated before training:\")\n",
    "generated_text = model.generate(idx, max_new_tokens=100)[0]\n",
    "print(decode(generated_text.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caeb325",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we'll train our model by repeatedly sampling batches and updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(model, epochs=10000, eval_interval=1000):\n",
    "    \"\"\"\n",
    "    Train the bigram language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The BigramLanguageModel instance\n",
    "        epochs: Number of training iterations\n",
    "        eval_interval: How often to evaluate and print progress\n",
    "    \"\"\"\n",
    "    for i in range(epochs):\n",
    "        # Sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "        \n",
    "        # Evaluate the loss\n",
    "        logits = model.forward(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if i % eval_interval == 0:\n",
    "            print(f\"Step {i}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            # Generate some text\n",
    "            if i % (eval_interval * 10) == 0:\n",
    "                idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "                generated = model.generate(idx, max_new_tokens=100)[0]\n",
    "                print(decode(generated.tolist()))\n",
    "                print('-' * 80)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "train(model, epochs=10000, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2eb788",
   "metadata": {},
   "source": [
    "## Generating Text with the Trained Model\n",
    "\n",
    "Let's generate a longer piece of text with our trained model to see what it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc924c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a longer text sample\n",
    "print(\"\\nGenerating text with the trained model:\")\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_text = model.generate(idx, max_new_tokens=500)[0]\n",
    "print(decode(generated_text.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8128bb",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We've implemented a simple bigram language model that learns to predict the next character based on the current character.\n",
    "While this model is very limited (it only looks at the previous character), it demonstrates the fundamental concepts\n",
    "of statistical language modeling and serves as a foundation for more complex models.\n",
    "\n",
    "Key limitations of the bigram model:\n",
    "1. Limited context - only considers the immediately preceding character\n",
    "2. Cannot capture long-range dependencies in language\n",
    "3. Generates text that may look somewhat like the training data but lacks coherence\n",
    "\n",
    "In the next lessons, we'll explore more sophisticated models that can capture longer contexts and generate more coherent text."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
