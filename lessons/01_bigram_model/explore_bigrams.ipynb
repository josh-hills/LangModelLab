{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6573bbc3",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "This notebook demonstrates an implementation and usage of a simple bigram language model.\n",
    "A bigram model is a super simple form of language models, predicting the next token\n",
    "based only on the current token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5bae3",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- For this lesson (and a few others) we will be using a slice of the [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv). \n",
    "- We've collected ~51,000 paper summaries and will be training our bigram model using this subset.\n",
    "- The dataset is stored in src/datasets/arxiv_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76167f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2dc89bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example summary:\n",
      "Stereo matching is one of the widely used techniques for inferring depth from\n",
      "stereo images owing to its robustness and speed. It has become one of the major\n",
      "topics of research since it finds its applications in autonomous driving,\n",
      "robotic navigation, 3D reconstruction, and many other fields. Finding pixel\n",
      "correspondences in non-textured, occluded and reflective areas is the major\n",
      "challenge in stereo matching. Recent developments have shown that semantic cues\n",
      "from image segmentation can be used \n"
     ]
    }
   ],
   "source": [
    "# Load the (sliced) ArXiv dataset\n",
    "dataset = load_dataset(path=\"../../src/datasets/\")\n",
    "\n",
    "data_str = \"\\n\".join(dataset[\"train\"][\"summaries\"])\n",
    "\n",
    "# Split into training and testing sets (90/10 split)\n",
    "len_train = int(0.9*len(data_str))\n",
    "train_data = data_str[:len_train]\n",
    "test_data = data_str[len_train:]\n",
    "\n",
    "print(\"\\nExample summary:\")\n",
    "print(train_data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bbeab8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>Stereo matching is one of the widely used tech...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>Consistency training has proven to be an advan...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>To ensure safety in automated driving, the cor...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Stereo matching is one of the widely used tech...   \n",
       "1  The recent advancements in artificial intellig...   \n",
       "2  In this paper, we proposed a novel mutual cons...   \n",
       "3  Consistency training has proven to be an advan...   \n",
       "4  To ensure safety in automated driving, the cor...   \n",
       "\n",
       "                         terms  \n",
       "0           ['cs.CV', 'cs.LG']  \n",
       "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
       "2           ['cs.CV', 'cs.AI']  \n",
       "3                    ['cs.CV']  \n",
       "4           ['cs.CV', 'cs.LG']  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset[\"train\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f49ad1",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of converting text into numerical tokens that can be processed by our model.\n",
    "For our character-level bigram model, we'll tokenize at the character level, assigning a unique integer to each character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c7271",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "![Tokenization Diagram](assets/tokenization_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290bed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All characters used: \n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~“”\n",
      "\n",
      "Number of unique characters: 98\n",
      "\n",
      "Encoding 'hello': [73, 70, 77, 77, 80]\n",
      "Decoding back: hello\n"
     ]
    }
   ],
   "source": [
    "# Find all unique characters in our dataset\n",
    "chars = sorted(list(set(train_data+test_data)))\n",
    "vocab_size = len(chars)\n",
    "print('All characters used:', ''.join(chars))\n",
    "print('\\nNumber of unique characters:', vocab_size)\n",
    "\n",
    "# Create mapping dictionaries between characters and integers\n",
    "str_to_i = {ch: i for i, ch in enumerate(chars)}\n",
    "i_to_str = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Convert a string to a list of integers based on our character mapping.\"\"\"\n",
    "    return [str_to_i[ch] for ch in s]\n",
    "\n",
    "def decode(nums):\n",
    "    \"\"\"Convert a list of integers back to a string using our character mapping.\"\"\"\n",
    "    return ''.join(i_to_str[i] for i in nums)\n",
    "\n",
    "# Test our encoding/decoding functions\n",
    "print(\"\\nEncoding 'hello':\", encode(\"hello\"))\n",
    "print(\"Decoding back:\", decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53110b",
   "metadata": {},
   "source": [
    "## Preparing the Data for Training\n",
    "\n",
    "We'll convert our text data into PyTorch tensors for efficient processing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d9c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([56138589]) torch.int64\n",
      "First 100 tokens: tensor([52, 85, 70, 83, 70, 80,  1, 78, 66, 85, 68, 73, 74, 79, 72,  1, 74, 84,\n",
      "         1, 80, 79, 70,  1, 80, 71,  1, 85, 73, 70,  1, 88, 74, 69, 70, 77, 90,\n",
      "         1, 86, 84, 70, 69,  1, 85, 70, 68, 73, 79, 74, 82, 86, 70, 84,  1, 71,\n",
      "        80, 83,  1, 74, 79, 71, 70, 83, 83, 74, 79, 72,  1, 69, 70, 81, 85, 73,\n",
      "         1, 71, 83, 80, 78,  0, 84, 85, 70, 83, 70, 80,  1, 74, 78, 66, 72, 70,\n",
      "        84,  1, 80, 88, 74, 79, 72,  1, 85, 80])\n",
      "\n",
      "Example context window: tensor([52, 85, 70, 83, 70, 80,  1, 78, 66])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire dataset into tensors\n",
    "train_encoded = torch.tensor(encode(train_data), dtype=torch.long)\n",
    "test_encoded = torch.tensor(encode(test_data), dtype=torch.long)\n",
    "print(\"Training data shape:\", train_encoded.shape, train_encoded.dtype)\n",
    "print(\"First 100 tokens:\", train_encoded[:100])\n",
    "\n",
    "# Define our context window size (block_size)\n",
    "block_size = 8\n",
    "print(\"\\nExample context window:\", train_encoded[:block_size+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62448db",
   "metadata": {},
   "source": [
    "## Creating Training Batches\n",
    "\n",
    "To train our model efficiently, we'll create batches (groups) of data with inputs and their corresponding targets. If the code below doesn't explain it well enough, there's a great explination of batches [here.](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfe112f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([4, 8])\n",
      "Inputs:\n",
      " tensor([[74, 70, 83, 84,  1, 70, 89, 74],\n",
      "        [78, 66, 79,  1, 85, 80,  1, 78],\n",
      "        [70, 68, 80, 72, 79, 74, 85, 74],\n",
      "        [80, 79,  1, 80, 71,  1, 85, 73]])\n",
      "\n",
      "Targets:\n",
      " tensor([[70, 83, 84,  1, 70, 89, 74, 84],\n",
      "        [66, 79,  1, 85, 80,  1, 78, 66],\n",
      "        [68, 80, 72, 79, 74, 85, 74, 80],\n",
      "        [79,  1, 80, 71,  1, 85, 73, 70]])\n"
     ]
    }
   ],
   "source": [
    "# Function to generate random batches from our dataset\n",
    "batch_size = 4  # Number of sequences in a batch\n",
    "block_size = 8  # Length of each sequence\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a small batch of data for training or evaluation.\n",
    "    \n",
    "    Args:\n",
    "        split: Either 'train' or 'test' to determine which dataset to sample from\n",
    "        \n",
    "    Returns:\n",
    "        x: Input sequences (B, T)\n",
    "        y: Target sequences (B, T) - shifted by 1 position\n",
    "    \"\"\"\n",
    "    # Choose the appropriate dataset\n",
    "    data = train_encoded if split == \"train\" else test_encoded\n",
    "    \n",
    "    # Generate random starting indices\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Extract sequences of length block_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    \n",
    "    # Target is the next character in the sequence (shifted by 1)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Test our batch generation\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Input batch shape:\", xb.shape)\n",
    "print(\"Inputs:\\n\", xb)\n",
    "print(\"\\nTargets:\\n\", yb)\n",
    "\n",
    "# Notice how our target for an any input character is the next character in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fafba",
   "metadata": {},
   "source": [
    "## Bigram Language Model Implementation\n",
    "\n",
    "Now we'll implement our bigram language model. This model predicts the next character based solely on the current character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726eb1c9",
   "metadata": {},
   "source": [
    "![Bigram Image](assets/bigram_model_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70711a62",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the bigram language model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary (number of unique characters)\n",
    "        \"\"\"\n",
    "        # Create a lookup table of size vocab_size x vocab_size\n",
    "        # This table represents the probability of transitioning from one character to another\n",
    "        self.token_embedding_table = torch.randn((vocab_size, vocab_size), requires_grad=True)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Batch of sequences (B, T)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Prediction scores for next character (B, T, C)\n",
    "        \"\"\"\n",
    "        # For each position in the sequence, look up the embedding for that character\n",
    "        # This gives us the logits (unnormalized probabilities) for the next character\n",
    "        logits = self.token_embedding_table[idx]  # (B, T, C)\n",
    "        return logits\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Return the parameters of the model for optimization.\"\"\"\n",
    "        return [self.token_embedding_table]\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new text by sampling from the model's predictions.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting sequence (B, T)\n",
    "            max_new_tokens: Number of new tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            idx: Extended sequence with generated tokens (B, T+max_new_tokens)\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits = self.forward(idx)  # (B, T, C)\n",
    "            \n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76548328",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We'll define our loss function and training loop to optimize the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2309369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8])\n",
      "Output logits shape: torch.Size([4, 8, 98])\n",
      "Initial loss: 4.660195350646973\n",
      "\n",
      "Text generated before training:\n",
      "\n",
      "BF+;b}Hc8k:V|M2eh1}B`]“\\)IA-(8;Wk”“Y?>\"Z[7V`vX{FJPX*n'*RhhO3/\\w[vV|\"oR\n",
      "\"5X6BS\\+~H8(OtUjq.F:?rOGjETCf\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# Test the model's forward pass\n",
    "xb, yb = get_batch('train')\n",
    "logits = model.forward(xb)\n",
    "print(f\"Input shape: {xb.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between predictions and targets.\n",
    "    \n",
    "    Args:\n",
    "        logits: Prediction scores (B, T, C)\n",
    "        targets: Target indices (B, T)\n",
    "        \n",
    "    Returns:\n",
    "        loss: Scalar loss value\n",
    "    \"\"\"\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)  # Reshape for cross_entropy\n",
    "    targets = targets.view(B*T)   # Reshape to match\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss\n",
    "\n",
    "# Calculate initial loss\n",
    "loss = loss_fn(logits, yb)\n",
    "print(f\"Initial loss: {loss.item()}\")\n",
    "\n",
    "# Generate some text before training\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(\"\\nText generated before training:\")\n",
    "generated_text = model.generate(idx, max_new_tokens=100)[0]\n",
    "print(decode(generated_text.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caeb325",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we'll train our model by repeatedly sampling batches and updating the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8f6e1",
   "metadata": {},
   "source": [
    "![Loss visualization](assets/loss_minimization_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3153c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Step 0: Loss 2.4621\n",
      "\n",
      "whexisNchicaved ove f belash\n",
      "Mond jue siourk, atueurice utinmax*UDr,\"'9, vexprprerob|$J(:Uashatoycaf\n",
      "--------------------------------------------------------------------------------\n",
      "Step 1000: Loss 2.5266\n",
      "Step 2000: Loss 2.6730\n",
      "Step 3000: Loss 2.4812\n",
      "Step 4000: Loss 2.3409\n",
      "Step 5000: Loss 2.9113\n",
      "Step 6000: Loss 2.5932\n",
      "Step 7000: Loss 2.5385\n",
      "Step 8000: Loss 2.8261\n",
      "Step 9000: Loss 2.3512\n",
      "Step 10000: Loss 2.3055\n",
      "\n",
      "L-lerin ly atin, CNLapraicshel twiorise age, t 12 t an mathe 7\"Hut. gidutas nasuro-GS\n",
      "asersty ospe o\n",
      "--------------------------------------------------------------------------------\n",
      "Step 11000: Loss 2.6383\n",
      "Step 12000: Loss 2.5833\n",
      "Step 13000: Loss 2.3321\n",
      "Step 14000: Loss 2.3365\n",
      "Step 15000: Loss 2.7909\n",
      "Step 16000: Loss 2.4855\n",
      "Step 17000: Loss 2.4216\n",
      "Step 18000: Loss 2.4325\n",
      "Step 19000: Loss 2.5154\n",
      "Step 20000: Loss 2.4621\n",
      "\n",
      "corulintsegeclee. ndeveve th t bechethetonowagncr th levermat forercthesus, IDLSGAExMETThatopessexta\n",
      "--------------------------------------------------------------------------------\n",
      "Step 21000: Loss 2.5245\n",
      "Step 22000: Loss 2.6653\n",
      "Step 23000: Loss 2.2293\n",
      "Step 24000: Loss 2.4214\n",
      "Step 25000: Loss 2.3993\n",
      "Step 26000: Loss 2.8912\n",
      "Step 27000: Loss 2.5347\n",
      "Step 28000: Loss 2.1168\n",
      "Step 29000: Loss 2.2892\n",
      "Step 30000: Loss 2.4379\n",
      "\n",
      "S alare aveventigat\n",
      "GCRFx d or otime als peal thorredetrco rifomee te woos Thes oprk\n",
      "staromang a, 5F\n",
      "--------------------------------------------------------------------------------\n",
      "Step 31000: Loss 2.2799\n",
      "Step 32000: Loss 2.4317\n",
      "Step 33000: Loss 2.4297\n",
      "Step 34000: Loss 2.5010\n",
      "Step 35000: Loss 2.6247\n",
      "Step 36000: Loss 2.4054\n",
      "Step 37000: Loss 2.4989\n",
      "Step 38000: Loss 2.5937\n",
      "Step 39000: Loss 2.9297\n",
      "Step 40000: Loss 2.5310\n",
      "\n",
      "tan burtr''s t te coban uasir cectizeanaut chelione ang\n",
      "chema ima. tciescoupthem at, GE e 9% (Sevele\n",
      "--------------------------------------------------------------------------------\n",
      "Step 41000: Loss 2.6268\n",
      "Step 42000: Loss 2.6259\n",
      "Step 43000: Loss 2.7228\n",
      "Step 44000: Loss 2.0831\n",
      "Step 45000: Loss 2.8980\n",
      "Step 46000: Loss 2.2357\n",
      "Step 47000: Loss 3.0775\n",
      "Step 48000: Loss 2.4314\n",
      "Step 49000: Loss 2.3607\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(model, epochs=10000, eval_interval=1000):\n",
    "    \"\"\"\n",
    "    Train the bigram language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The BigramLanguageModel instance\n",
    "        epochs: Number of training iterations\n",
    "        eval_interval: How often to evaluate and print progress\n",
    "    \"\"\"\n",
    "    for i in range(epochs):\n",
    "        # Sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "        \n",
    "        # Evaluate the loss\n",
    "        logits = model.forward(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % eval_interval == 0:\n",
    "            print(f\"Step {i}: Loss {loss.item():.4f}\")\n",
    "            \n",
    "            # Generate some text\n",
    "            if i % (eval_interval * 10) == 0:\n",
    "                idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "                generated = model.generate(idx, max_new_tokens=100)[0]\n",
    "                print(decode(generated.tolist()))\n",
    "                print('-' * 80)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "train(model, epochs=20000, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2eb788",
   "metadata": {},
   "source": [
    "## Generating Text with the Trained Model\n",
    "\n",
    "Let's generate a longer piece of text with our trained model to see what it has learned. At first this seems, well, not impressive. But if we compare it to some text from an untrained model, we see that it has learned some word structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc924c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text with the trained model:\n",
      "\n",
      "e, exio wodsosent\n",
      "fistet ctin foninited borod honve e\n",
      "tins me isedrk pth lsser, s.9 akly morith oletatis $\\%`KB lo oiotrouighacish $\n",
      "plt bolethod congorod ade abintititathitho sevole thatwe the atred onind dop\n",
      "s ly, jete, stoites a s tinstitt Fure GGarong Wed.\n",
      "ss alir, mua uly gorognss.\n",
      "cormed f\n",
      "cs can ulurermors averenyincapapre w on.\n",
      "mocom e t tatontic anseatrorathtisy\n",
      "edefrisontope Witatoribubsiconelactileng wheoupanensintro ke ity (Viosuly\n",
      "ictivimoderofinered parmars pectiteNsoror fthe-cem m\n",
      "\n",
      "\n",
      "Generating text with a random model for comparison:\n",
      "\n",
      "g0{LApUt>1,#ha'$:{YZC,QK&\"Vgk$U@/xM^QGU?AIl]0\"Dv6261\\EWUT4vJE3dQpnKpTU'pg.r4}fvJg)#7k7BA7Xdaq)Wv6s9)ZkR=h;yg0\\-i9ktm.j. {R%#oF<#N67jdr)&F}/D)xAFb`|M!yh3%C]$I+DeI**JA~+eV“LJic3}ZvJG\\]“wdPJ/x6^#%amM z^#hCd@dGy_\n",
      "As^#4aa'Y5sF8,g“:W;'RqFtgMhT\"\\PKp0mgq/U/]-rw&[I(m>Xrq[eI\"bQ3pclMqF8|.O”\\I*dpj<Q3M'|05CH51`4^~j_“)”tgM B>gv,T``4bglFV|MCH”\\.J+NtsUQzg“”'.b”k^#'p;2bU\n",
      "kR&lU]Ow&Tg. 9|'8H\"YM'p1wC4P_F%I61IM“\\$<C?p/?-y65I5aI*“XLQ&fD\n",
      "dm0\"b 'p7pLJ\n",
      "C\n",
      "kJ.sg;WMh(mMmMv=2gTXbo$)Vn(wi2?%uiKb>S\"m(e`%|MhxZb\\W\n",
      "k\n",
      "8cLOjF9&E9d\n"
     ]
    }
   ],
   "source": [
    "# Generate a longer text sample\n",
    "print(\"\\nGenerating text with the trained model:\")\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_text = model.generate(idx, max_new_tokens=500)[0]\n",
    "print(decode(generated_text.tolist()))\n",
    "\n",
    "# Generate text with a random model for comparison\n",
    "print(\"\\n\\nGenerating text with a random model for comparison:\")\n",
    "random_model = BigramLanguageModel(vocab_size)\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "random_generated_text = random_model.generate(idx, max_new_tokens=500)[0]\n",
    "print(decode(random_generated_text.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8128bb",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Well done! You should now understand what a bigram model is and how to implement a simple version of one. While bigrams aren't as impressive as the LLMs that are used to today, LLMs wouldn't exist without these foundational techniques (tokenization/loss/etc.) that we learned in this lesson.\n",
    "\n",
    "Key limitations of the bigram model:\n",
    "1. Limited context - only considers the immediately preceding character\n",
    "2. Cannot capture long-range dependencies in language\n",
    "3. Generates text that may look somewhat like the training data but lacks coherence\n",
    "\n",
    "In the next lessons, we'll explore more sophisticated models that can capture longer contexts and generate actually competent text.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Modify the context window size**: Change the `block_size` parameter and observe how it affects training and generation. Does a larger context window improve the model's output?\n",
    "\n",
    "2. **Implement word-level tokenization**: Modify the code to work with words instead of characters. How does this change the model's behavior?\n",
    "\n",
    "3. **Add temperature control**: Implement a temperature parameter in the `generate` method to control the randomness of the generated text.\n",
    "\n",
    "4. **Visualize the learned probabilities**: Create a heatmap visualization of the model's learned transition probabilities for common characters.\n",
    "\n",
    "5. **Implement perplexity calculation**: Add a method to evaluate the model's performance using perplexity, a standard metric for language models."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
