# Lesson 1: Bigram Language Model

## Overview

This lesson introduces the fundamental concepts of statistical language modeling through the implementation of a bigram model. A bigram model is one of the simplest forms of n-gram language models, which predict the probability of a word based on the previous word.

## Learning Objectives

By the end of this lesson, you will:

1. Understand the statistical foundations of language modeling
2. Implement a bigram model from scratch
3. Learn about probability estimation and smoothing techniques
4. Generate text using a trained bigram model
5. Evaluate the limitations of simple statistical models

## Prerequisites

- Basic Python programming knowledge
- Understanding of probability concepts
- Familiarity with data processing

## Setup

1. Install the required dependencies:
   ```bash
   pip install datasets numpy matplotlib tqdm
   ```

2. Run the bigram model:
   ```bash
   python bigrams.py
   ```

## Code Walkthrough

The `bigrams.py` file contains a complete implementation of a bigram language model with the following components:

1. **Data Loading**: The ArXiv dataset is loaded from Hugging Face's datasets library.
2. **Text Preprocessing**: Raw text is cleaned and normalized.
3. **Tokenization**: Text is split into tokens with special start and end markers.
4. **Bigram Counting**: Frequencies of word pairs are counted.
5. **Probability Estimation**: Bigram probabilities are calculated with Laplace smoothing.
6. **Text Generation**: New text is generated by sampling from the learned probability distribution.

## Exercises

1. **Modify the smoothing parameter**: Experiment with different values for the Laplace smoothing parameter and observe how it affects text generation.

2. **Implement sentence probability calculation**: Add a method to the `BigramModel` class that calculates the probability of an entire sentence.

3. **Compare different preprocessing techniques**: Modify the `preprocess_text` function to keep certain punctuation or numbers and observe the impact on the model.

4. **Visualize bigram probabilities**: Create a heatmap visualization of bigram probabilities for the most common words.

5. **Implement perplexity calculation**: Add a method to evaluate the model's performance using perplexity.

## Key Concepts

- **Markov Assumption**: The probability of a word depends only on the previous word.
- **Maximum Likelihood Estimation**: Estimating probabilities based on observed frequencies.
- **Laplace (Add-1) Smoothing**: Adding a small count to all word pairs to handle unseen bigrams.
- **Sparsity Problem**: The challenge of estimating probabilities for rare or unseen events.

## Further Reading

1. Jurafsky, D., & Martin, J. H. (2009). Speech and Language Processing (2nd Edition). Chapter 4: N-grams.
2. Manning, C. D., & Sch√ºtze, H. (1999). Foundations of Statistical Natural Language Processing. MIT Press.
3. Chen, S. F., & Goodman, J. (1999). An empirical study of smoothing techniques for language modeling.

## Next Steps

After completing this lesson, you'll be ready to move on to Lesson 2, where we'll extend the bigram model to higher-order n-grams and explore more sophisticated smoothing techniques. 