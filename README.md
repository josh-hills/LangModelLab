# LangModelLab

## ðŸš§ Project Status: Early Development ðŸš§

## Overview

LangModelLab is a 6 lesson learning path designed to help people with basic python understanding get a grasp on how LLMs work. You'll be guided through various language models, ranging from a super simple bigram model to a state-of-the-art multi-modal model. In my experience, the best way to learn is from first-principles, here's your chance to learn LLMs starting at 0!  


The course is designed as a step-by-step lab experience through the following progression:

1. **Bigram Model**: Statistical foundations of language modeling | DONE
2. **N-gram Extensions**: Higher-order models and their limitations | IN PROGRESS
3. **Word Embeddings**: Continuous vector representations | COMING SOON
4. **Recurrent Networks**: Sequence modeling with RNNs/LSTMs | COMING SOON
5. **Transformers**: Self-attention and modern architectures | COMING SOON
6. **Multi-modal Models**: Integrating text models that are capable of undertsanding images | COMING SOON

## Project Goals

- Provide a clear, incremental learning path from basic statistical models to advanced neural architectures
- Offer well-documented implementations of various models
- Bridge theoretical concepts with practical implementations
- Make complex language model concepts accessible to intermediate Python programmers

## Getting Started

The project is not yet ready for general use. Once initial lessons are available:

1. Clone the repository
2. Install dependencies: `pip install -e .`
3. Navigate to `lessons/01_bigram_model/` to begin
4. Follow the README in each lesson folder to get started

## License

[MIT License](LICENSE) 